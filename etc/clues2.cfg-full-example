#####################################################################################################################
#
# CLUES Runtime
#
#####################################################################################################################

[client]

# Secret token to allow external XMLRPC calls
CLUES_SECRET_TOKEN=8e0eadc543eef7bca47fefb4

# URL for the CLUES XMLRPC server
CLUES_XMLRPC=http://localhost:8000/RPC2

# Timeout when waiting a request to be served
CLUES_REQUEST_WAIT_TIMEOUT=300

# Log file for the clients
LOG_FILE=/var/log/clues2/clues2-cli.log

# Log level for the clients: debug, info, warning, error
LOG_LEVEL=debug

[general]

# CLUES will consider any *.cfg file inside CONFIG_DIR in the CLUES config directory
# CLUES considers (in this order) /etc/clues2/ ~/clues2/etc/ and ./etc/ folders to contain the clues2.cfg file
CONFIG_DIR=conf.d

# Seconds in which CLUES will issue a control line in the log file (to make sure that it continues working)
LOGGER_MARK=1800

# Path to the database that it is used by CLUES. You can also use a sqlite db (format: sqlite:///filename.db) or a mysql db (format: mysql://user:password@host/database)
DB_CONNECTION_STRING=sqlite:///var/lib/clues2/clues.db

# Space separated list of hosts that will not be considered by CLUES (e.g. node1 node2)
DISABLED_HOSTS=

# True to load the state of the nodes from the database on startup. Otherwise CLUES will not have any information until the infrastructure is monitorized
RETRIEVE_NODES_FROM_DB_ON_STARTUP=True

# Seconds to wait for the execution of commands e.g. command to power on a node or to monitorize a lrms
TIMEOUT_COMMANDS=10

# Secret token to allow external XMLRPC calls
CLUES_SECRET_TOKEN=8e0eadc543eef7bca47fefb4

# Port in which the CLUES server will listen
CLUES_PORT=8000

# Path to the log file for CLUES (leave it in blank to get output on screen; useful for debugging)
LOG_FILE=/var/log/clues2/clues2.log

# Log level to output in the log file: debug, info, warning, error
LOG_LEVEL=debug

# The python module that CLUES will use to monitorize the LRMS. It MUST have a "lrms" class inside and it MUST be accesible by python as an import
# * Tip: if neeeded, you can modify the cluesd executable to include the path in which the module is using sys.path.append("/path/to/module")
LRMS_CLASS=cluesplugins.pbs

# The python module that CLUES will use to power on or off the internal nodes. It MUST have a "powermanager" class inside and it MUST be accesible by python as an import
# * Tip: if neeeded, you can modify the cluesd executable to include the path in which the module is using sys.path.append("/path/to/module")
POWERMANAGER_CLASS=cluesplugins.ipmi

# Host in which CLUES is 
CLUES_HOST=

[monitoring]
# Max time to wait to power on a node. Once passed this time, if the monitor still reports a off state, CLUES will consider that the power-on command for the node has failed
MAX_WAIT_POWERON=300

# Max time to wait to power off a node. Once passed this time, if the monitor still reports a on state, CLUES will consider that the power-off command for the node has failed
MAX_WAIT_POWEROFF=300

# Seconds between monitoring the nodes (i.e. calls to the LRMS_CLASS.lrms.get_nodeinfo method)
# * 0 to deactivate
PERIOD_MONITORING_NODES=5

# Seconds between monitoring the jobs (i.e. calls to the LRMS_CLASS.lrms.get_jobinfo method)
# * 0 to deactivate. Deactivated by default
PERIOD_MONITORING_JOBS=0

# Seconds to call to the lifecycle for the classes of the powermanager or the lrms
PERIOD_LIFECYCLE=5

# Seconds of grace for fails on monitoring the nodes. If a monitorization fails, the previous monitorization will be considered valid up to this value.
PERIOD_MONITORING_NODES_FAIL_GRACE=120

# Seconds of grace for fails on monitoring the jobs. If a monitorization fails, the previous monitorization will be considered valid up to this value.
PERIOD_MONITORING_JOBS_FAIL_GRACE=120

# If a lrms reports a negative value, should it be considered as literal or as infinite resources?
NEGATIVE_RESOURCES_MEANS_INFINITE=True

# Period of time while a node will not be considered on even if it is reported to be on by the lrms (e.g. the lrms monitoring system has noticed that the node is on but it cannot execute jobs, yet)
DELAY_POWON=10

# Period of time while a node will not be considered off even if it is reported to be off by the lrms (e.g. we have just powered off the node and the lrms monitoring system have not noticed it yet)
DELAY_POWOFF=10

# Period of time while the jobs will remain in the monitoring system even if they have dissapeared from the lrms monitorization
COOLDOWN_SERVED_JOBS=120

# Period of time while the request will remain in the monitoring system even if they have been served
COOLDOWN_SERVED_REQUESTS=120

[scheduling]

# Seconds between calls to the schedulers pipeline
PERIOD_SCHEDULE=5

# Maximum number of nodes that will be booting simultaneously
MAX_BOOTING_NODES=2

# Comma separated list for the python classes that are used as schedulers. The scheduling pipeline will be called in the same order that are stated here. Each class MUST have a "schedule" method inside and the class MUST be accesible by python as an import
# * Tip: if neeeded, you can modify the cluesd executable to include the path in which the module is using sys.path.append("/path/to/module")
SCHEDULER_CLASSES=clueslib.schedulers.CLUES_Scheduler_PowOn_Requests, CLUES_Scheduler_Reconsider_Jobs, clueslib.schedulers.CLUES_Scheduler_PowOff_IDLE

# Setting for the recovery of nodes
# * Not working yet...
RETRIES_POWER_ON=3
PERIOD_RECOVERY_NODES=30

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# Settings for the clueslib.schedulers.CLUES_Scheduler_PowOff_IDLE
#
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[scheduler_power_off_idle]

# Time (in seconds) to consider that a node that has remain idle can be powered off
IDLE_TIME=1800

# Seconds that must pass before considering a node to be powered off (e.g. it has just been powered off and it is idle again)
COOLDOWN_NODES=0

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# Settings for the clueslib.schedulers.CLUES_Scheduler_PowOn_Requests
#
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[scheduler_requests]

# Time to allocate the resources for a request that have been served. To take into account the reservation in the period while the job is effectively released to the queue and it is scheduled.
# * The problem of the 'second'
ALLOCATE_SERVED_REQUESTS=1

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# Settings for the clueslib.schedulers.CLUES_Scheduler_Reconsider_Jobs
#
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[scheduler_reconsider_jobs]

# Time that a job has to be monitorized in pending state, before a new request for resources is created
RECONSIDER_JOB_TIME=60                 

#####################################################################################################################
#
# Wrappers
#
#####################################################################################################################

[ONE WRAPPER]
ONE_XMLRPC=http://localhost:2633/RPC2
ONE_AUTH=clues:cluespass
ONE_XMLRPC_TIMEOUT=10

[SGE]
CLUES_SECRET_TOKEN=8e0eadc543eef7bca47fefb4
CLUES_XMLRPC=http://localhost:8000/RPC2
SGE_QCONF_COMMAND=all.q
SGE_ROOT=/var/lib/gridengine
SGE_DEFAULT_QUEUE=all.q

#####################################################################################################################
#
# Power Managers
#
#####################################################################################################################

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings for creating and managing a Virtual Cluster using the IM. The settings refer to use IM as a power manager. So by making use of this power manager, 
# the working nodes of the cluster are supposed to be VMs that are dynamically created using CLUES. These settings refer only to powering on or off the nodes. Please refer
# to the LRMS settings to specify how to check the state of the nodes to decide wich one to power on or off.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[IM VIRTUAL CLUSTER]
IM_VIRTUAL_CLUSTER_INFID=0
IM_VIRTUAL_CLUSTER_XMLRPC=http://localhost:8899
IM_VIRTUAL_CLUSTER_XMLRCP_SSL_CA_CERTS=''
IM_VIRTUAL_CLUSTER_XMLRCP_SSL=False
IM_VIRTUAL_CLUSTER_AUTH_DATA_FILE=/usr/local/ec3/auth.dat
IM_VIRTUAL_CLUSTER_DROP_FAILING_VMS=30
IM_VIRTUAL_CLUSTER_FORGET_MISSING_VMS=30

# Pay fraction in the cloud deployment (in seconds)
IM_VIRTUAL_CLUSTER_VM_PAY_FRACTION = 0

# Time margin to terminate the instances (in seconds)
IM_VIRTUAL_CLUSTER_VM_TIME_MARGIN = 0

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings for creating and managing a Virtual Cluster using ONE. The settings refer to use ONE as a power manager. So by making use of this power manager, 
# the working nodes of the cluster are supposed to be VMs that are dynamically created using CLUES. These settings refer only to powering on or off the nodes. Please refer
# to the LRMS settings to specify how to check the state of the nodes to decide wich one to power on or off.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[ONE VIRTUAL CLUSTER]

# The XMLRPC of the ONE server that will launch the virtual working nodes
ONE_VIRTUAL_CLUSTER_XMLRPC=http://localhost:2633/RPC2

# The credentials to launch the virtual working nodes
ONE_VIRTUAL_CLUSTER_AUTH=clues:cluespass

# Timeout for the XMLRPC to respond to connections (to avoid CLUES to be hang on a XMLRPC query)
ONE_VIRTUAL_CLUSTER_XMLRPC_TIMEOUT=60

# The file in /etc/hosts format that contains the correspondence of the ONE IP addresses to the host names in the LRMS (it may be interchangeable by the standard /etc/hosts file of the front-end)
ONE_VIRTUAL_CLUSTER_HOSTS_FILE=virtualonecluster.hosts

# The template ID that will be used to instantiate VMs that will act as working nodes
ONE_VIRTUAL_CLUSTER_TEMPLATE_ID=1

# The ONE template for the VM, following the ONE syntax. Please beware of the indentation to define the multiline template. Otherwise it will fail.
# * You can use
#   %%a as a substitution for the address
#   %%h as a substitution for the hostname
ONE_VIRTUAL_CLUSTER_TEMPLATE=
    NAME="%%h"
    CPU="1"
    DISK=[IMAGE_ID="1"]
    FEATURES=[ACPI="yes"]
    GRAPHICS=[LISTEN="0.0.0.0",TYPE="VNC",KEYMAP="es"]
    MEMORY="512"
    NIC=[IP="%%a",NETWORK_ID="14"]
    OS=[ARCH="x86_64"]
    VCPU="1"

# When a VM is not detected by the monitoring system of CLUES but the ONE connector thinks that it has been powered on by him, it will be powered off once passed this time
ONE_VIRTUAL_CLUSTER_DROP_FAILING_VMS=-1

# If a VM is not detected by the monitoring system of CLUES nor it is detected by ONE, it will be discarded after this time. It is introduced to avoid discarding VM info before fully powering off the VM
# * this is an advanced setting
ONE_VIRTUAL_CLUSTER_FORGET_MISSING_VMS=30

# The ONE connector tries to periodically guess which of the running VMs in ONE are part of the Virtual Cluster. This is made by checking the IP addresses of the VMs. Tries to capture hand-deployed VMs.
# This setting is the period of time that the ONE connector will try to guess if the VMs are part from the Virtual Cluster.
ONE_VIRTUAL_CLUSTER_GUESS_VMS_TIME=30

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings for powering on or off nodes using a Wake-on-Lan based and SSH mechanism. You will need to make sure that WOL is properly working and then set the
# commands to power on the working nodes. Powering off the nodes is supposed to be done by issuing the poweroff command in the node by ssh. The command is issued as it is
# stated in the configuration, so you can check it by issuing it in the commandline.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[WOL]
# The file in /etc/hosts format-like that contains the correspondence of the MAC addresses to the host names in the LRMS
WOL_HOSTS_FILE=wol.hosts

# The commandlines that must be used to power on and off the nodes using WOL (CLUES will subsitute the %%a as the MAC address of the node, using print function)
#
# * you can use %%a to substitute the MAC address and %%h to substitute the hostname
WOL_CMDLINE_POWON=/sbin/ether-wake %%a
WOL_CMDLINE_POWOFF=ssh %%h power off

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings for powering on or off nodes using a IPMI mechanism. You will need to make sure that IPMI is properly working and then set the commands to power
# on or off the working nodes. Please make sure that you have created and set the proper passwords. The command is issued as it is stated in the configuration, so you can
# check it by issuing it in the commandline.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[IPMI]
# The file in /etc/hosts format that contains the correspondence of the IPMI IP addresses to the host names in the LRMS
IPMI_HOSTS_FILE=ipmi.hosts

# The commandlines that must be used to power on and off the nodes using IPMI (CLUES will subsitute the %s as the name of the node, using print function)
#
# * you can use %%a to substitute the MAC address and %%h to substitute the hostname
IPMI_CMDLINE_POWON=/usr/bin/ipmitool -I lan -H %%a -P "" power on
IPMI_CMDLINE_POWOFF=/usr/bin/ipmitool -I lan -H %%a -P "" power off

#####################################################################################################################
#
# LRMS
#
#####################################################################################################################

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings to consult the state of the nodes of a ONE deployment in which CLUES is automating power (is powering on or off the internal nodes). These
# settings are only to check the state of the nodes. Please refer to the powermanager settings to specify how to power on or off the nodes.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[ONE LRMS]
# The address of the ONE XML-RPC server
ONE_XMLRPC=http://localhost:2633/RPC2

# The credentials for the ONE user that will be used to access to ONE
ONE_AUTH=clues:cluespass

# Timeout for contacting the ONE XML-RPC server
ONE_XMLRPC_TIMEOUT=180

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings to consult the state of the nodes of a SGE deployment in which CLUES is automating power (is powering on or off the internal nodes). These
# settings are only to check the state of the nodes. Please refer to the powermanager settings to specify how to power on or off the nodes.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[SGE]

# Command used to get the hosts in the queue
SGE_QHOST_COMMAND=/usr/bin/qhost

# Command to get the configuration of sge
SGE_QCONF_COMMAND=/usr/bin/qconf

# Root path for SGE
SGE_ROOT=/var/lib/gridengine

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings to consult the state of the nodes of a PBS deployment in which CLUES is automating power (is powering on or off the internal nodes). These
# settings are only to check the state of the nodes. Please refer to the powermanager settings to specify how to power on or off the nodes.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[PBS]

# Name of the PBS server
PBS_SERVER=localhost

# Command to get the state of the queue
PBS_QSTAT_COMMAND=/usr/bin/qstat

# Command to get the information about the nodes
PBS_PBSNODES_COMMAND=/usr/bin/pbsnodes

# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#
# These are the settings to consult the state of the nodes of a SLURM deployment in which CLUES is automating power (is powering on or off the internal nodes). These
# settings are only to check the state of the nodes. Please refer to the powermanager settings to specify how to power on or off the nodes.
# 
# -------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[SLURM]
CLUES_SECRET_TOKEN=
SLURM_SERVER=slurmserverpublic
SLURM_PARTITION_COMMAND=/usr/local/bin/scontrol -o show partitions
SLURM_NODES_COMMAND=/usr/local/bin/scontrol -o show nodes
SLURM_JOBS_COMMAND=/usr/local/bin/scontrol -o show jobs
SLURM_DEFAULT_QUEUE=wn
