[general]
CONFIG_DIR=plugins.d

LOGGER_MARK=600
TIMEOUT_COMMANDS=10

# Database connection string (use sqlite://filename or mysql://user@host/dbname)
DB_CONNECTION_STRING=sqlite:///home/calfonso/clues2/var/clues.db

# These hosts will not be managed by CLUES (i.e. will not be powered on or off)
DISABLED_HOSTS=fc-one01

LOG_FILE=/home/calfonso/clues2/var/clues2.log
LOG_FILE= # ./var/clues2.log
LOG_LEVEL=debug ; info, warning, error, debug

CLUES_SECRET_TOKEN=8e0eadc543eef7bca47fefb4
CLUES_PORT=8000
CLUES_HOST=localhost

LRMS_CLASS=cluesplugins.pbs
POWERMANAGER_CLASS=
# POWERMANAGER_CLASS=cluesplugins.one
# POWERMANAGER_CLASS=cluesplugins.onetemplate

# SCHEDULERS=clueslib.schedulers.

[client]

# Log file for the CLI commands
LOG_FILE=/home/calfonso/clues2/var/clues2.log

# Parameters to connect to the server
CLUES_REMOTE_SERVER_HOST=localhost
CLUES_REMOTE_SERVER_PORT=8000
CLUES_XMLRPC=http://localhost:8000/RPC2

# Secret key to use when connecting to the server
CLUES_SECRET_TOKEN=8e0eadc543eef7bca47fefb4
CLUES_REMOTE_SERVER_INSECURE=false

[scheduling]

PERIOD_RECOVERY_NODES=10

# The frequence in which the scheduler pipeline is executed
PERIOD_SCHEDULE=1
MAX_BOOTING_NODES=2

# The python classes that are used as schedulers. Take into account that the scheduling pipeline will invoke them in the same order that are stated here
SCHEDULER_CLASSES=clueslib.schedulers.CLUES_Scheduler_PowOn_Requests, clueslib.schedulers.CLUES_Scheduler_PowOff_IDLE
#, clueslib.schedulers_extra.CLUES_Scheduler_PowOn_Free

# The maximum time that the requests should be alive. In case that the request passes this time, it will dissapear
REQUESTS_LIFETIME=600

[scheduler_power_off_idle]

# The time that a node has to be in idle state to be considered that it can be powered off
IDLE_TIME=6000
IDLE_TIME=120
IDLE_TIME=600

# The time that a node must be in IDLE state before it is considered to be powered off again
COOLDOWN_NODES=60 ; 120

[scheduler_requests]

# When a request is served, it will remain a certain time for the allocation system. This should be made because when a request is served, the monitoring system of the LRMS does not
# take it into account immediately, so we must allocate the resources for a while. Tries to solve "el problema del segundo".
# * The best setting is to put the LRMS scheduling period
COOLDOWN_SERVED_REQUESTS=30

[monitoring]

# The amount of time that a node can be waited to be on or off once its power on or off request is executed
MAX_WAIT_POWERON=180
MAX_WAIT_POWEROFF=120

# When a node is in pow_on state, we will wait up to DELAY_POWON seconds before changing its state to ON. This tries to avoid glitches
DELAY_POWON=10
DELAY_POWOFF=10

# The frequence in which the nodes are monitorized (i.e. the LRMS is contacted to get the information about the nodes)
PERIOD_MONITORING_NODES=20
# Time that failures in monitoring is forgiven
PERIOD_MONITORING_NODES_FAIL_GRACE=120
PERIOD_MONITORING_JOBS_FAIL_GRACE=120
# The frequence in which the queue (i.e. the LRMS is contacted to get the information about the jobs in the queue)
PERIOD_MONITORING_JOBS=50
# The frequence in which the system calls to the lifecycle mechanism of the platform (and so, to the lifecycle of the lrms and the power manager)
# PERIOD_LIFECYCLE=5

# When the monitoring system obtains a number of resources (e.g memory or cpu) that is negative, it means that it has infinite resources.
# This is used to workaround the fact that pbsnodes does not obtain information about the memory of nodes if they have not been powered on
# at least once while pbs-server was runnning. In those cases, the connector are commited to set available memory to a ficticious value, or
# to not setting that value. In such cases this setting will have sense: if the monitor set to a negative value (e.g. -1) CLUES may power
# on the node and the monitor will get its values.
NEGATIVE_RESOURCES_MEANS_INFINITE=True
